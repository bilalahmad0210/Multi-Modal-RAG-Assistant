# ğŸ§  Multi-Modal RAG Assistant (Fully Local, LLM + OCR)

This is a **fully offline AI assistant** that can analyze and reason over:
- ğŸ“„ PDFs
- ğŸ–¼ï¸ Images (with OCR text extraction)
- ğŸ¥ Videos / Audio (via transcription)
- ğŸ’¬ Natural language questions

All powered by local models (no internet or API keys needed).

---

## ğŸš€ Features

| Modality | What it can do |
|----------|----------------|
| ğŸ“„ PDF | Answer questions from uploaded PDFs or preloaded documents |
| ğŸ–¼ï¸ Image | Extract image captions and visible text (OCR) |
| ğŸ™ï¸ Audio / Video | Transcribe and answer questions about content |
| ğŸ¤– Local LLM | Uses Mistral 7B (GGUF) via `llama-cpp-python` |
| ğŸ”’ Offline | 100% privacy-safe and free. No API keys required |

---

## ğŸ§° Tech Stack

- **Gradio** â€“ for UI
- **LangChain + FAISS** â€“ for text & video retrieval
- **SentenceTransformers** â€“ for vector embeddings
- **Mistral-7B Instruct (GGUF)** â€“ for smart answers via `llama-cpp-python`
- **Tesseract OCR** â€“ to read text from images (e.g., tweets, screenshots)
- **BLIP** â€“ for image captioning
- **Whisper** â€“ for video/audio transcription

---

## ğŸ“¦ Installation

### 1. Clone the repo
```bash
git clone https://github.com/your-username/multi-modal-rag-assistant.git
cd multi-modal-rag-assistant
```

### 2. Set up environment
```bash
python -m venv env
env\Scripts\activate
pip install -r requirements.txt
```

### 3. Download models

#### ğŸ”¡ Sentence Embedding
```bash
# Automatically downloaded: all-MiniLM-L6-v2
```

#### ğŸ§  LLM (Mistral 7B GGUF)
Download from: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF  
Put the `.gguf` file in:
```
models/mistral-7b-instruct.Q4_K_M.gguf
```

#### ğŸ™ï¸ Whisper (Transcription)
```bash
pip install git+https://github.com/openai/whisper.git
```

#### ğŸ”  OCR (Tesseract)
- Install from: https://github.com/UB-Mannheim/tesseract/wiki
- Add install path (e.g., `C:\Program Files\Tesseract-OCR`) to System Environment Variables

---

## âœ… How to Run

```bash
python app.py
```

Visit: http://localhost:7860  
Ask a question and upload a file!

---

## ğŸ§ª Example Prompts

| File | Prompt |
|------|--------|
| Resume.pdf | "What are this candidate's top skills?" |
| Tweet image | "What does the tweet say?" |
| Job PDF | "Summarize the job requirements." |
| Audio.mp3 | "What is this person talking about?" |

---

## ğŸ“ Folder Structure

```
multi_modal_assistant/
â”œâ”€â”€ app.py
â”œâ”€â”€ llm.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral-7b-instruct.Q4_K_M.gguf
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ text.py
â”‚   â”œâ”€â”€ image_utils.py
â”‚   â””â”€â”€ video.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ videos/
â”‚   â””â”€â”€ pdfs/
â””â”€â”€ README.md
```

---

## âš ï¸ Limitations

- High RAM (14GB+) recommended for local LLMs
- First inference may take a few seconds
- OCR accuracy depends on image quality
- FAISS index is in-memory and non-persistent by default

---

## ğŸ§  Roadmap

- [ ] Streamed LLM responses
- [ ] Chat memory and history
- [ ] Agent-based tool use (LangGraph)
- [ ] Upload multiple files
- [ ] Desktop or web API version

---

## ğŸ§‘â€ğŸ’» Author

**Bilal Ahmad** â€“ AI Engineer, Builder, Learner  
> Powered by Mistral, LangChain, BLIP, Whisper, Tesseract, and Python ğŸ

---

## ğŸ›¡ï¸ License

MIT â€“ use this assistant freely, locally, and privately.
